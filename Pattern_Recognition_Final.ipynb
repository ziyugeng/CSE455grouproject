{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Load all the necessary functions"
      ],
      "metadata": {
        "id": "LJLks1ivtlPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TfErQcl66_yl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. import CNN/Daily Mail dataset, from https://huggingface.co/datasets/cnn_dailymail"
      ],
      "metadata": {
        "id": "bbWP2rA3uDUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# follow the instruction from https://huggingface.co/docs/datasets-server/quick_start\n",
        "# Ziyu Geng\n",
        "\n",
        "API_URL = \"https://datasets-server.huggingface.co/splits?dataset=cnn_dailymail\"  # contains train, val, and test\n",
        "def query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "data = query()\n",
        "print(data)\n",
        "\n",
        "# train\n",
        "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=cnn_dailymail&config=1.0.0&split=train&offset=0&limit=100\"  # train\n",
        "def train_query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "train_data = train_query()\n",
        "print(train_data)\n",
        "\n",
        "# val\n",
        "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=cnn_dailymail&config=1.0.0&split=validation&offset=0&limit=100\"  # validation\n",
        "def val_query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "val_data = val_query()\n",
        "print(val_data)\n",
        "\n",
        "# test\n",
        "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=cnn_dailymail&config=1.0.0&split=test&offset=0&limit=100\"  # test\n",
        "def test_query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "test_data = test_query()\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "1-U7rEkVuDnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### this is the english version if we find an english version of the dataset\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#1 tokenize text to individual words or subwords, remove stopwords, punctuation and special characters, and make them all lowercase\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # tokenize text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # remove punctuation and special characters\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [word.translate(table) for word in words]\n",
        "\n",
        "    # convert words to lowercase\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # remove empty strings/single characters\n",
        "    words = [word for word in words if len(word) > 1]\n",
        "\n",
        "    return words\n",
        "\n",
        "#2 pad/truncate text to make all samples have the same length\n",
        "\n",
        "def pad_or_truncate_text(text, max_length):\n",
        "    # preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # truncate text if longer than max_length\n",
        "    if len(preprocessed_text) > max_length:\n",
        "        preprocessed_text = preprocessed_text[:max_length]\n",
        "\n",
        "    # pad text if shorter than max_length\n",
        "    elif len(preprocessed_text) < max_length:\n",
        "        padding_length = max_length - len(preprocessed_text)\n",
        "        preprocessed_text.extend(['<PAD>'] * padding_length)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "max_length = 800  # <-----should be roughly how many words/tokens are in each article\n",
        "\n",
        "#\n",
        "for example in train_data['rows']:\n",
        "    article_text = example['row']['article']\n",
        "    preprocessed_text = pad_or_truncate_text(article_text, max_length)\n",
        "    print(preprocessed_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5lj9zMcIoDBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7r8Ae0IwmL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####this is the dutch version#############\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#1 tokenize text to individual words or subwords, remove stopwords, punctuation and special characters, and make them all lowercase\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # tokenize text into words\n",
        "    words = word_tokenize(text, language='dutch')\n",
        "\n",
        "    # remove punctuation and special characters\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [word.translate(table) for word in words]\n",
        "\n",
        "    # convert words to lowercase\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('dutch'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # remove empty strings/single characters\n",
        "    words = [word for word in words if len(word) > 1]\n",
        "\n",
        "    return words\n",
        "\n",
        "#2 pad/truncate text to make all samples have the same length\n",
        "\n",
        "def pad_or_truncate_text(text, max_length):\n",
        "    # preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # truncate text if longer than max_length\n",
        "    if len(preprocessed_text) > max_length:\n",
        "        preprocessed_text = preprocessed_text[:max_length]\n",
        "\n",
        "    # pad text if shorter than max_length\n",
        "    elif len(preprocessed_text) < max_length:\n",
        "        padding_length = max_length - len(preprocessed_text)\n",
        "        preprocessed_text.extend(['<PAD>'] * padding_length)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "max_length = 800  # <-----should be roughly how many words/tokens are in each article\n",
        "\n",
        "#\n",
        "for example in train_data['rows']:\n",
        "    article_text = example['row']['article']\n",
        "    preprocessed_text = pad_or_truncate_text(article_text, max_length)\n",
        "    print(preprocessed_text)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fObzu0Dgwm1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent text as word embeddigs\n",
        "\n",
        "# -Word emeddings examples are Word2Vec, GloVe, FastText and represent each word as dense vector\n",
        "# Ziyu Geng, follow https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html.\n",
        "\n",
        "# sorry guys, I did not find the Word2Vec pytorch material, so I did not use it, if you guys find something error, feel free to correct.\n",
        "\n",
        "embedding_dim = 10 # same number from https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# list of train texts\n",
        "train_preprocessed_text = []\n",
        "for example in train_data['rows']:\n",
        "  article_text = example['row']['article']\n",
        "  train_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "# build vocabulary\n",
        "voc = set()\n",
        "for example in train_data['rows']:\n",
        "  article_text = example['row']['article']\n",
        "  preprocessed_text = pad_or_truncate_text(article_text, max_length)\n",
        "  for word in preprocessed_text:\n",
        "      voc.add(word)\n",
        "\n",
        "# make word to number\n",
        "word_to_num = {}\n",
        "i = 0\n",
        "for word in voc:\n",
        "    word_to_num[word] = i\n",
        "    i += 1\n",
        "\n",
        "class wordembedding(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(wordembedding, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.embeddings(inputs)\n",
        "\n",
        "# Create the model and the optimizer\n",
        "word_model = wordembedding(len(voc), embedding_dim)\n",
        "optimizer = optim.SGD(word_model.parameters(), lr=0.001)\n",
        "\n",
        "# loop train_preprocessed_text and generate embeddings\n",
        "for sentence in train_preprocessed_text:\n",
        "  sentence_idxs = []\n",
        "  for word in sentence:\n",
        "    id = word_to_num[word]  # Get the index of the word\n",
        "    sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "  sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "  word_model.zero_grad()\n",
        "  embeddings = word_model(sentence_idxs)\n",
        "\n",
        "  print(embeddings)\n"
      ],
      "metadata": {
        "id": "Br1C0TEegt3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define NN architecture\n",
        "\n",
        "# some suggested are RNN, LSTM, or Transformer based models like BERT or GPT but im pretty sure we can use bert bc its a pre trained model?\n",
        "\n"
      ],
      "metadata": {
        "id": "QFDixK8oh9iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train model"
      ],
      "metadata": {
        "id": "lCVHAHzeiKyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MdcOibo8iK-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JYyZhfggt7o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}