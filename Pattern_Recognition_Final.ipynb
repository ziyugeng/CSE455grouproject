{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Load all the necessary functions"
      ],
      "metadata": {
        "id": "LJLks1ivtlPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TfErQcl66_yl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. import CNN/Daily Mail dataset, from https://huggingface.co/datasets/cnn_dailymail"
      ],
      "metadata": {
        "id": "bbWP2rA3uDUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# follow the instruction from https://huggingface.co/docs/datasets-server/quick_start\n",
        "# Ziyu Geng\n",
        "\n",
        "# This is English version now, sorry for import wrong dataset.\n",
        "\n",
        "API_URL = \"https://datasets-server.huggingface.co/splits?dataset=cnn_dailymail\"  # contains train, val, and test\n",
        "def query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "data = query()\n",
        "print(data)\n",
        "\n",
        "# train\n",
        "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=cnn_dailymail&config=1.0.0&split=train&offset=0&limit=100\"  # train\n",
        "def train_query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "train_data = train_query()\n",
        "print(train_data)\n",
        "\n",
        "# val\n",
        "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=cnn_dailymail&config=1.0.0&split=validation&offset=0&limit=100\"  # validation\n",
        "def val_query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "val_data = val_query()\n",
        "print(val_data)\n",
        "\n",
        "# test\n",
        "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=cnn_dailymail&config=1.0.0&split=test&offset=0&limit=100\"  # test\n",
        "def test_query():\n",
        "    response = requests.get(API_URL)\n",
        "    return response.json()\n",
        "test_data = test_query()\n",
        "print(test_data)"
      ],
      "metadata": {
        "id": "1-U7rEkVuDnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#1 tokenize text to individual words or subwords, remove stopwords, punctuation and special characters, and make them all lowercase\n",
        "vocabulary = set()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # tokenize text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # remove punctuation and special characters\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [word.translate(table) for word in words]\n",
        "\n",
        "    # convert words to lowercase\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # remove empty strings/single characters\n",
        "    words = [word for word in words if len(word) > 1]\n",
        "\n",
        "    global vocabulary\n",
        "    vocabulary.update(words)\n",
        "\n",
        "    return words\n",
        "\n",
        "#2 pad/truncate text to make all samples have the same length\n",
        "\n",
        "def pad_or_truncate_text(text, max_length):\n",
        "    # preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # truncate text if longer than max_length\n",
        "    if len(preprocessed_text) > max_length:\n",
        "        preprocessed_text = preprocessed_text[:max_length]\n",
        "\n",
        "    # pad text if shorter than max_length\n",
        "    elif len(preprocessed_text) < max_length:\n",
        "        padding_length = max_length - len(preprocessed_text)\n",
        "        preprocessed_text.extend(['<PAD>'] * padding_length)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "max_length = 800  # <-----should be roughly how many words/tokens are in each article\n",
        "\n",
        "#\n",
        "train_summaries = [example['row']['highlights'] for example in train_data['rows']]\n",
        "val_summaries = [example['row']['highlights'] for example in val_data['rows']]\n",
        "test_summaries = [example['row']['highlights'] for example in test_data['rows']]\n",
        "\n",
        "# Preprocess target summaries for each split\n",
        "train_targets = [preprocess_text(summary) for summary in train_summaries]\n",
        "val_targets = [preprocess_text(summary) for summary in val_summaries]\n",
        "test_targets = [preprocess_text(summary) for summary in test_summaries]\n",
        "\n",
        "train_texts = []\n",
        "val_texts = []\n",
        "test_texts = []\n",
        "\n",
        "for example in train_data['rows']:\n",
        "    article_text = example['row']['article']\n",
        "    preprocessed_text = pad_or_truncate_text(article_text, max_length)\n",
        "    train_texts.append(preprocessed_text)\n",
        "\n",
        "for example in test_data['rows']:\n",
        "    article_text = example['row']['article']\n",
        "    preprocessed_text = pad_or_truncate_text(article_text, max_length)\n",
        "    test_texts.append(preprocessed_text)\n",
        "\n",
        "for example in val_data['rows']:\n",
        "    article_text = example['row']['article']\n",
        "    preprocessed_text = pad_or_truncate_text(article_text, max_length)\n",
        "    val_texts.append(preprocessed_text)\n",
        "\n",
        "vocabulary = set()\n",
        "for text in train_texts:\n",
        "    vocabulary.update(text)\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "print(train_texts)\n",
        "print(val_texts)\n",
        "print(test_texts)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5lj9zMcIoDBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent text as word embeddigs\n",
        "\n",
        "# -Word emeddings examples are Word2Vec, GloVe, FastText and represent each word as dense vector\n",
        "# Ziyu Geng, follow https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html.\n",
        "\n",
        "# sorry guys, I did not find the Word2Vec pytorch material, so I did not use it, if you guys find something error, feel free to correct.\n",
        "\n",
        "embedding_dim = 200\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# list of train texts\n",
        "train_preprocessed_text = []\n",
        "for example in train_data['rows']:\n",
        "  article_text = example['row']['article']\n",
        "  train_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "for i, sentence in enumerate(train_preprocessed_text):\n",
        "    print(f\"DEBUG_OUTPUT: Training Example {i + 1}:\")\n",
        "    print(sentence)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "val_preprocessed_text = []\n",
        "for example in val_data['rows']:\n",
        "  article_text = example['row']['article']\n",
        "  val_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "for i, sentence in enumerate(val_preprocessed_text):\n",
        "    print(f\"DEBUG_OUTPUT: Validation Example {i + 1}:\")\n",
        "    print(sentence)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# list of test texts\n",
        "test_preprocessed_text = []\n",
        "for example in test_data['rows']:\n",
        "  article_text = example['row']['article']\n",
        "  test_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "for i, sentence in enumerate(test_preprocessed_text):\n",
        "    print(f\"DEBUG_OUTPUT: Testing Example {i + 1}:\")\n",
        "    print(sentence)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "# build vocabulary\n",
        "voc = set()\n",
        "all_texts = train_preprocessed_text + val_preprocessed_text + test_preprocessed_text\n",
        "for sentence in all_texts:\n",
        "    for word in sentence:\n",
        "        voc.add(word)\n",
        "\n",
        "\n",
        "# make word to number\n",
        "word_to_num = {}\n",
        "i = 0\n",
        "for word in voc:\n",
        "    word_to_num[word] = i\n",
        "    i += 1\n",
        "\n",
        "class wordembedding(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(wordembedding, self).__init__()\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.embeddings(inputs)\n",
        "\n",
        "# Create the model and the optimizer\n",
        "word_model = wordembedding(len(voc), embedding_dim)\n",
        "optimizer = optim.SGD(word_model.parameters(), lr=0.001)\n",
        "\n",
        "# loop train_preprocessed_text and generate embeddings\n",
        "for sentence in train_preprocessed_text:\n",
        "  sentence_idxs = []\n",
        "  for word in sentence:\n",
        "    id = word_to_num[word]  # Get the index of the word\n",
        "    sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "  sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "  word_model.zero_grad()\n",
        "  embeddings = word_model(sentence_idxs)\n",
        "\n",
        "  print(embeddings)\n",
        "\n",
        "for sentence in val_preprocessed_text:\n",
        "   sentence_idxs = []\n",
        "   for word in sentence:\n",
        "    id = word_to_num[word]  # Get the index of the word\n",
        "    sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "   sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "   word_model.zero_grad()\n",
        "   embeddings = word_model(sentence_idxs)\n",
        "\n",
        "   print(embeddings)\n",
        "\n",
        "\n",
        "for sentence in test_preprocessed_text:\n",
        "   sentence_idxs = []\n",
        "   for word in sentence:\n",
        "    id = word_to_num[word]  # Get the index of the word\n",
        "    sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "   sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "   word_model.zero_grad()\n",
        "   embeddings = word_model(sentence_idxs)\n",
        "\n",
        "   print(embeddings)\n",
        "\n",
        "train_embeddings_list = []\n",
        "val_embeddings_list = []\n",
        "test_embeddings_list = []\n",
        "\n",
        "# loop train_preprocessed_text and generate embeddings\n",
        "for sentence in train_preprocessed_text:\n",
        "    sentence_idxs = []\n",
        "    for word in sentence:\n",
        "        id = word_to_num[word]  # Get the index of the word\n",
        "        sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "    sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    word_model.zero_grad()\n",
        "    embeddings = word_model(sentence_idxs)\n",
        "    train_embeddings_list.append(embeddings.detach())  # Store the embeddings\n",
        "\n",
        "# loop val_preprocessed_text and generate embeddings\n",
        "for sentence in val_preprocessed_text:\n",
        "    sentence_idxs = []\n",
        "    for word in sentence:\n",
        "        id = word_to_num[word]  # Get the index of the word\n",
        "        sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "    sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    word_model.zero_grad()\n",
        "    embeddings = word_model(sentence_idxs)\n",
        "    val_embeddings_list.append(embeddings.detach())  # Store the embeddings\n",
        "\n",
        "# loop test_preprocessed_text and generate embeddings\n",
        "for sentence in test_preprocessed_text:\n",
        "    sentence_idxs = []\n",
        "    for word in sentence:\n",
        "        id = word_to_num[word]  # Get the index of the word\n",
        "        sentence_idxs.append(id)  # Append the index to the list\n",
        "\n",
        "    sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    word_model.zero_grad()\n",
        "    embeddings = word_model(sentence_idxs)\n",
        "    test_embeddings_list.append(embeddings.detach())  # Store the embeddings\n",
        "\n",
        "# Convert lists to PyTorch tensors\n",
        "train_embeddings_tensor = torch.stack(train_embeddings_list)\n",
        "val_embeddings_tensor = torch.stack(val_embeddings_list)\n",
        "test_embeddings_tensor = torch.stack(test_embeddings_list)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Br1C0TEegt3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "KN0X7MAdorkG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define NN architecture\n",
        "\n",
        "# some suggested are RNN, LSTM, or Transformer based models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "embedding_dim = 10\n",
        "hidden_dim = 256\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "        embedded_input = self.embedding(input_sequence)\n",
        "        encoder_outputs, (hidden_state, cell_state) = self.rnn(embedded_input)\n",
        "        return encoder_outputs, (hidden_state, cell_state)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        max_len = encoder_outputs.size(1)\n",
        "        h = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
        "        encoder_outputs = encoder_outputs.transpose(1, 2)\n",
        "        attn_scores = F.softmax(torch.bmm(h, self.attn(encoder_outputs)), dim=2)\n",
        "        context = torch.bmm(attn_scores, encoder_outputs).squeeze(1)\n",
        "        return context\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence, hidden, encoder_outputs):\n",
        "        embedded_input = self.embedding(input_sequence)\n",
        "        context = self.attention(hidden, encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded_input, context.unsqueeze(1)), dim=2)\n",
        "        decoder_outputs, hidden = self.rnn(rnn_input, hidden)\n",
        "        output_sequence = F.log_softmax(self.fc(decoder_outputs.squeeze(1)), dim=1)\n",
        "        return output_sequence, hidden\n",
        "\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(Seq2SeqAttention, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = Encoder(vocab_size, embedding_dim, hidden_dim)\n",
        "        self.decoder = Decoder(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, source_sequence, target_sequence):\n",
        "        embedded_source = self.embedding(source_sequence)\n",
        "        embedded_target = self.embedding(target_sequence)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(embedded_source)\n",
        "        batch_size, target_length = target_sequence.size(0), target_sequence.size(1)\n",
        "        outputs = torch.zeros(batch_size, target_length, vocab_size).to(target_sequence.device)\n",
        "        decoder_input = embedded_target[:, 0].unsqueeze(1)\n",
        "        for t in range(1, target_length):\n",
        "            output, hidden = self.decoder(decoder_input, hidden, encoder_outputs)\n",
        "            outputs[:, t] = output\n",
        "            decoder_input = embedded_target[:, t].unsqueeze(1)\n",
        "        return outputs\n",
        "\n",
        "# train_embeddings_tensor, val_embeddings_tensor, test_embeddings_tensor\n",
        "\n",
        "def text_to_indices(text, word_to_num):\n",
        "    return [word_to_num[word] for word in text]\n",
        "\n",
        "train_indices = [text_to_indices(text, word_to_num) for text in train_texts]\n",
        "val_indices = [text_to_indices(text, word_to_num) for text in val_texts]\n",
        "test_indices = [text_to_indices(text, word_to_num) for text in test_texts]\n",
        "\n",
        "# Convert the lists of indices to tensors\n",
        "train_texts_tensor = torch.tensor(train_indices, dtype=torch.long)\n",
        "val_texts_tensor = torch.tensor(val_indices, dtype=torch.long)\n",
        "test_texts_tensor = torch.tensor(test_indices, dtype=torch.long)\n",
        "\n",
        "print(\"Train Texts Tensor Shape:\", train_texts_tensor.shape)\n",
        "print(\"Val Texts Tensor Shape:\", val_texts_tensor.shape)\n",
        "print(\"Test Texts Tensor Shape:\", test_texts_tensor.shape)\n",
        "\n",
        "train_input_sequences = train_texts_tensor[:, :-1]  # Remove the last token from the input\n",
        "train_target_sequences = train_texts_tensor[:, 1:]   # Shift the target by one time step\n",
        "\n",
        "val_input_sequences = val_texts_tensor[:, :-1]\n",
        "val_target_sequences = val_texts_tensor[:, 1:]\n",
        "\n",
        "test_input_sequences = test_texts_tensor[:, :-1]\n",
        "test_target_sequences = test_texts_tensor[:, 1:]\n",
        "\n",
        "\n",
        "final_train_dataset = TensorDataset(train_embeddings_tensor)\n",
        "final_val_dataset = TensorDataset(val_embeddings_tensor)\n",
        "final_test_dataset = TensorDataset(test_embeddings_tensor)\n",
        "\n",
        "print(\"Final Train Dataset Size:\", len(final_train_dataset))\n",
        "print(\"Final Val Dataset Size:\", len(final_val_dataset))\n",
        "print(\"Final Test Dataset Size:\", len(final_test_dataset))\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(final_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(final_val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(final_test_dataset, batch_size=batch_size, shuffle = False)\n",
        "\n",
        "# Verify Vocabulary Size and Indices\n",
        "print(\"Vocabulary Size:\", vocab_size)\n",
        "print(\"Train Texts Sample:\", train_texts[:2])\n",
        "print(\"Val Texts Sample:\", val_texts[:2])\n",
        "print(\"Test Texts Sample:\", test_texts[:2])\n",
        "\n",
        "print(\"Train Indices Sample:\", train_indices[:2])  # Verify the index representation of the texts\n",
        "\n",
        "# Inspect Input Sequences and Targets\n",
        "print(\"Train Texts Tensor Shape:\", train_texts_tensor.shape)\n",
        "print(\"Val Texts Tensor Shape:\", val_texts_tensor.shape)\n",
        "print(\"Test Texts Tensor Shape:\", test_texts_tensor.shape)\n",
        "\n",
        "data = next(iter(train_loader))\n",
        "print(\"Train Batch Shape:\", data[0].shape)\n",
        "\n",
        "# instantiate model\n",
        "vocab_size = 9094\n",
        "model = Seq2SeqAttention(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "QFDixK8oh9iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 20\n",
        "plot_interval = 5\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "train_accuracy_list = []\n",
        "val_accuracy_list = []\n",
        "\n",
        "def calculate_accuracy(output_sequence, target_sequence):\n",
        "    _, predicted = torch.max(output_sequence, 1)\n",
        "    correct = (predicted == target_sequence).sum().item()\n",
        "    total = target_sequence.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "\n",
        "    # train\n",
        "    for total_batch in tqdm(train_loader, desc=\"Training Batches\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Unpack the total_batch into source and target\n",
        "        source_batch, target_batch = total_batch\n",
        "\n",
        "        source_sequence = source_batch.long()\n",
        "        target_sequence = target_batch.long()\n",
        "\n",
        "        max_index_source = torch.max(source_sequence).item()\n",
        "        print(\"Max index in source_sequence:\", max_index_source)\n",
        "        max_index_target = torch.max(target_sequence).item()\n",
        "        print(\"Max index in target_sequence:\", max_index_target)\n",
        "        print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "        output_sequence = model(source_sequence, target_sequence)\n",
        "\n",
        "        max_index = torch.max(source_sequence).item()\n",
        "        print(\"Max index:\", max_index)\n",
        "\n",
        "\n",
        "        # flatten\n",
        "        output_sequence = output_sequence.view(-1, vocab_size)\n",
        "        target_sequence = target_batch.view(-1)\n",
        "\n",
        "        loss = criterion(output_sequence, target_sequence)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += calculate_accuracy(output_sequence, target_sequence)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_accuracy = total_accuracy / len(train_loader)\n",
        "    train_loss_list.append(avg_loss)\n",
        "    train_accuracy_list.append(avg_accuracy)\n",
        "\n",
        "    # val loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_val_loss = 0.0\n",
        "        total_val_accuracy = 0.0\n",
        "\n",
        "        for source_batch, target_batch in tqdm(val_loader, desc=\"Validation Batches\", leave=False):\n",
        "\n",
        "            source_sequence = source_batch.long()\n",
        "            target_sequence = target_batch.long()\n",
        "\n",
        "            max_index_source = torch.max(source_sequence).item()\n",
        "            print(\"Max index in source_sequence:\", max_index_source)\n",
        "            max_index_target = torch.max(target_sequence).item()\n",
        "            print(\"Max index in target_sequence:\", max_index_target)\n",
        "            print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "            output_sequence = model(source_sequence, target_sequence)\n",
        "\n",
        "            # flatten\n",
        "            output_sequence = output_sequence.view(-1, vocab_size)\n",
        "            target_sequence = target_batch.view(-1)\n",
        "\n",
        "            val_loss = criterion(output_sequence, target_sequence)\n",
        "            total_val_loss += val_loss.item()\n",
        "            total_val_accuracy += calculate_accuracy(output_sequence, target_sequence)\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        avg_val_accuracy = total_val_accuracy / len(val_loader)\n",
        "        val_loss_list.append(avg_val_loss)\n",
        "        val_accuracy_list.append(avg_val_accuracy)\n",
        "\n",
        "    # plot every 5 epochs\n",
        "    if (epoch + 1) % plot_interval == 0 or epoch == num_epochs - 1:\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Training Accuracy: {avg_accuracy:.4f}, Val Accuracy: {avg_val_accuracy:.4f}\")\n",
        "\n",
        "# final graphs\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), train_loss_list, label='Training Loss')\n",
        "plt.plot(range(1, num_epochs + 1), val_loss_list, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), train_accuracy_list, label='Training Accuracy')\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracy_list, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "fQz61tScRUHl",
        "outputId": "d8ab55ca-47a8-404b-cdd0-14b538695f73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]\n",
            "Training Batches:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9ae819133f7d>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Unpack the total_batch into source and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msource_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0msource_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    }
  ]
}