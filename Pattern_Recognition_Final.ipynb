{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Load all the necessary functions"
      ],
      "metadata": {
        "id": "LJLks1ivtlPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfErQcl66_yl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "!pip install datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. import CNN/Daily Mail dataset, from https://huggingface.co/datasets/cnn_dailymail"
      ],
      "metadata": {
        "id": "bbWP2rA3uDUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Modify the dataset loading to load a smaller subset\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0', split=\"train[:500]\")  # Load only 500 examples for training\n",
        "\n",
        "# Train, validation, and test splits\n",
        "train_data = dataset\n",
        "val_data = load_dataset('cnn_dailymail', '3.0.0', split=\"validation[:100]\")  # Load 100 examples for validation\n",
        "test_data = load_dataset('cnn_dailymail', '3.0.0', split=\"test[:100]\")  # Load 100 examples for testing\n",
        "\n",
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "    return tokenized_sentences\n",
        "\n",
        "# Extract summaries and texts from each split\n",
        "train_summaries = [example['highlights'] for example in train_data]\n",
        "train_texts = [example['article'] for example in train_data]\n",
        "\n",
        "val_summaries = [example['highlights'] for example in val_data]\n",
        "val_texts = [example['article'] for example in val_data]\n",
        "\n",
        "test_summaries = [example['highlights'] for example in test_data]\n",
        "test_texts = [example['article'] for example in test_data]\n",
        "\n",
        "# Print the number of examples in each split\n",
        "print(\"Data Dimensions:\")\n",
        "print(\"Train data examples:\", len(train_data))\n",
        "print(\"Validation data examples:\", len(val_data))\n",
        "print(\"Test data examples:\", len(test_data))\n",
        "\n",
        "# Print the dimensions of target summaries and texts (articles)\n",
        "print(\"\\nTarget Summaries Dimensions:\")\n",
        "print(\"Train summaries:\", len(train_summaries))\n",
        "print(\"Validation summaries:\", len(val_summaries))\n",
        "print(\"Test summaries:\", len(test_summaries))\n",
        "\n",
        "print(\"\\nTexts (Articles) Dimensions:\")\n",
        "print(\"Train texts:\", len(train_texts))\n",
        "print(\"Validation texts:\", len(val_texts))\n",
        "print(\"Test texts:\", len(test_texts))\n"
      ],
      "metadata": {
        "id": "iFuCrCTui8VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import Counter\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "\n",
        "vocab_input = {'<OOV>': 0}\n",
        "word_counter_input = Counter(word for text in train_texts for word in nltk.word_tokenize(text))\n",
        "for word, count in word_counter_input.items():\n",
        "    if word not in vocab_input:\n",
        "        vocab_input[word] = len(vocab_input)\n",
        "\n",
        "# Create a vocabulary from the training set for target summaries\n",
        "vocab_target = {'<OOV>': 0}\n",
        "word_counter_target = Counter(word for summary in train_summaries for word in nltk.word_tokenize(summary))\n",
        "for word, count in word_counter_target.items():\n",
        "    if word not in vocab_target:\n",
        "        vocab_target[word] = len(vocab_target)\n",
        "\n",
        "# Function to convert a sentence to a sequence of integers for input text\n",
        "def sentence_to_sequence_input(sentence):\n",
        "    return [vocab_input[word] if word in vocab_input else vocab_input['<OOV>'] for word in sentence]\n",
        "\n",
        "# Function to convert a sentence to a sequence of integers for target summaries\n",
        "def sentence_to_sequence_target(sentence):\n",
        "    return [vocab_target[word] if word in vocab_target else vocab_target['<OOV>'] for word in sentence]\n",
        "\n",
        "# Convert the sentences in each split to sequences of integers for input text\n",
        "train_sequences_input = [sentence_to_sequence_input(text) for text in train_texts]\n",
        "val_sequences_input = [sentence_to_sequence_input(text) for text in val_texts]\n",
        "test_sequences_input = [sentence_to_sequence_input(text) for text in test_texts]\n",
        "\n",
        "# Convert the target summaries in each split to sequences of integers\n",
        "train_sequences_target = [sentence_to_sequence_target(summary) for summary in train_summaries]\n",
        "val_sequences_target = [sentence_to_sequence_target(summary) for summary in val_summaries]\n",
        "test_sequences_target = [sentence_to_sequence_target(summary) for summary in test_summaries]\n",
        "\n",
        "# Maximum length for each sentence\n",
        "max_length = 512\n",
        "max_summary_length = 100  # Set the desired maximum length for target summaries\n",
        "\n",
        "train_padded_input = rnn_utils.pad_sequence([torch.tensor(seq) for seq in train_sequences_input], batch_first=True, padding_value=0)\n",
        "val_padded_input = rnn_utils.pad_sequence([torch.tensor(seq) for seq in val_sequences_input], batch_first=True, padding_value=0)\n",
        "test_padded_input = rnn_utils.pad_sequence([torch.tensor(seq) for seq in test_sequences_input], batch_first=True, padding_value=0)\n",
        "\n",
        "# Pad the sequences for target summaries\n",
        "train_padded_target = rnn_utils.pad_sequence([torch.tensor(seq) for seq in train_sequences_target], batch_first=True, padding_value=0)\n",
        "val_padded_target = rnn_utils.pad_sequence([torch.tensor(seq) for seq in val_sequences_target], batch_first=True, padding_value=0)\n",
        "test_padded_target = rnn_utils.pad_sequence([torch.tensor(seq) for seq in test_sequences_target], batch_first=True, padding_value=0)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, sequences_input, sequences_target, max_length):\n",
        "        self.sequences_input = sequences_input\n",
        "        self.sequences_target = sequences_target\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences_input)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get the input sequence and target summary at the given index\n",
        "        input_sequence = self.sequences_input[index]\n",
        "        target_summary = self.sequences_target[index]\n",
        "\n",
        "        # Pad the sequences to the maximum length\n",
        "        input_sequence = self.pad_sequence(input_sequence, self.max_length)\n",
        "        target_summary = self.pad_sequence(target_summary, self.max_length)\n",
        "\n",
        "        # Convert the sequences to tensors\n",
        "        input_tensor = torch.tensor(input_sequence, dtype=torch.long)\n",
        "        target_tensor = torch.tensor(target_summary, dtype=torch.long)\n",
        "\n",
        "        return input_tensor, target_tensor\n",
        "\n",
        "    def pad_sequence(self, sequence, max_length):\n",
        "        if len(sequence) < max_length:\n",
        "            # Pad the sequence with zeros if its length is less than max_length\n",
        "            sequence = sequence + [0] * (max_length - len(sequence))\n",
        "        else:\n",
        "            # Truncate the sequence if its length exceeds max_length\n",
        "            sequence = sequence[:max_length]\n",
        "        return sequence\n",
        "\n",
        "# Create custom datasets\n",
        "train_dataset = CustomDataset(train_sequences_input, train_sequences_target, max_length)\n",
        "val_dataset = CustomDataset(val_sequences_input, val_sequences_target, max_length)\n",
        "test_dataset = CustomDataset(test_sequences_input, test_sequences_target, max_length)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Manually retrieve a batch of data from the train_loader\n",
        "for batch_inputs, batch_targets in train_loader:\n",
        "    # Print the dimensions of the tensors in the batch\n",
        "    print(\"Batch Input Dimensions:\", batch_inputs.shape)\n",
        "    print(\"Batch Target Dimensions:\", batch_targets.shape)\n",
        "    break  # Exit the loop after the first batch to avoid printing all batches\n",
        "\n",
        "\n",
        "# Define the Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "# Define the Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        # Encoder forward pass\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Decoder initial input (start token) and hidden states\n",
        "        input = trg[0, :]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "encoder_input_dim = len(vocab_input)\n",
        "decoder_output_dim = len(vocab_target)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "encoder = Encoder(encoder_input_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
        "decoder = Decoder(decoder_output_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)"
      ],
      "metadata": {
        "id": "I0djogvJpjXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import torch\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(preds, targets):\n",
        "    _, predicted = torch.max(preds, 1)\n",
        "    correct = (predicted == targets).sum().item()\n",
        "    return correct / len(targets)\n",
        "\n",
        "# Function to calculate loss\n",
        "def calculate_loss(preds, targets):\n",
        "    loss = criterion(preds, targets)\n",
        "    return loss.item()\n",
        "\n",
        "def train(model, num_epochs, train_loader, val_loader, print_every=5):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
        "        # Training\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        total_train_samples = 0\n",
        "        correct_train = 0\n",
        "\n",
        "        for batch_inputs, batch_targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_targets = batch_targets.to(device)\n",
        "            outputs = model(batch_inputs)\n",
        "            loss = criterion(outputs.view(-1, outputs.shape[-1]), batch_targets.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_train_loss += loss.item()\n",
        "            total_train_samples += len(batch_inputs) * batch_inputs.shape[1]\n",
        "            correct_train += (outputs.argmax(1) == batch_targets.view(-1)).sum().item()\n",
        "\n",
        "        train_loss = running_train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracy = correct_train / total_train_samples\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        total_val_samples = 0\n",
        "        correct_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_inputs, batch_targets in val_loader:\n",
        "                outputs = model(batch_inputs)\n",
        "                loss = criterion(outputs.view(-1, outputs.shape[-1]), batch_targets.view(-1))\n",
        "\n",
        "                running_val_loss += loss.item()\n",
        "                total_val_samples += len(batch_inputs) * batch_inputs.shape[1]\n",
        "                correct_val += (outputs.argmax(1) == batch_targets.view(-1)).sum().item()\n",
        "\n",
        "        val_loss = running_val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracy = correct_val / total_val_samples\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Print and plot results every 'print_every' epochs\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
        "            print(f\"Training Loss: {train_loss:.4f} - Training Accuracy: {train_accuracy:.4f}\")\n",
        "            print(f\"Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "            # Plot graphs\n",
        "            if epoch == num_epochs or epoch % (5 * print_every) == 0:\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                plt.subplot(1, 2, 1)\n",
        "                plt.plot(np.arange(1, epoch + 1), train_losses, label='Training Loss')\n",
        "                plt.plot(np.arange(1, epoch + 1), val_losses, label='Validation Loss')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.subplot(1, 2, 2)\n",
        "                plt.plot(np.arange(1, epoch + 1), train_accuracies, label='Training Accuracy')\n",
        "                plt.plot(np.arange(1, epoch + 1), val_accuracies, label='Validation Accuracy')\n",
        "                plt.xlabel('Epoch')\n",
        "                plt.ylabel('Accuracy')\n",
        "                plt.legend()\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PVmggBkU5Y_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import requests\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocessing functions\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    words = [word.translate(table) for word in words]\n",
        "    words = [word.lower() for word in words]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [word for word in words if len(word) > 1]\n",
        "    return words\n",
        "\n",
        "def pad_or_truncate_text(sentence, max_length):\n",
        "    if len(sentence) < max_length:\n",
        "        sentence = sentence + ['<PAD>'] * (max_length - len(sentence))\n",
        "    else:\n",
        "        sentence = sentence[:max_length]\n",
        "    return sentence\n",
        "\n",
        "def preprocess_target_summary(summary, max_length):\n",
        "    return pad_or_truncate_text(preprocess_text(summary), max_length)\n",
        "\n",
        "# ... (Previous code remains unchanged)\n",
        "\n",
        "# Fetch train data and target summaries\n",
        "train_data = fetch_data(TRAIN_API_URL)\n",
        "train_summaries = [example['row']['highlights'] for example in train_data['rows']]\n",
        "train_texts = [example['row']['article'] for example in train_data['rows']]\n",
        "\n",
        "# Fetch val data and target summaries\n",
        "val_data = fetch_data(VAL_API_URL)\n",
        "val_summaries = [example['row']['highlights'] for example in val_data['rows']]\n",
        "val_texts = [example['row']['article'] for example in val_data['rows']]\n",
        "\n",
        "# Fetch test data and target summaries\n",
        "test_data = fetch_data(TEST_API_URL)\n",
        "test_summaries = [example['row']['highlights'] for example in test_data['rows']]\n",
        "test_texts = [example['row']['article'] for example in test_data['rows']]\n",
        "\n",
        "# Preprocess article texts and target summaries for each split\n",
        "max_length = 512\n",
        "train_texts = [pad_or_truncate_text(preprocess_text(article_text), max_length) for article_text in train_texts]\n",
        "val_texts = [pad_or_truncate_text(preprocess_text(article_text), max_length) for article_text in val_texts]\n",
        "test_texts = [pad_or_truncate_text(preprocess_text(article_text), max_length) for article_text in test_texts]\n",
        "train_targets = [preprocess_target_summary(summary, max_length) for summary in train_summaries]\n",
        "val_targets = [preprocess_target_summary(summary, max_length) for summary in val_summaries]\n",
        "test_targets = [preprocess_target_summary(summary, max_length) for summary in test_summaries]\n",
        "\n",
        "\n",
        "# Print preprocessed data\n",
        "print(\"Train Texts:\", train_texts)\n",
        "print(\"Train Targets:\", train_targets)\n",
        "print(\"Val Texts:\", val_texts)\n",
        "print(\"Val Targets:\", val_targets)\n",
        "print(\"Test Texts:\", test_texts)\n",
        "print(\"Test Targets:\", test_targets)\n",
        "\n",
        "\n",
        "print(\"Train Texts Dimensions:\")\n",
        "for i, text in enumerate(train_texts):\n",
        "    print(f\"Text {i + 1} length:\", len(text))\n",
        "\n",
        "print(\"\\nTrain Targets Dimensions:\")\n",
        "for i, summary in enumerate(train_targets):\n",
        "    print(f\"Summary {i + 1} length:\", len(summary))\n",
        "\n",
        "print(\"\\nVal Texts Dimensions:\")\n",
        "for i, text in enumerate(val_texts):\n",
        "    print(f\"Text {i + 1} length:\", len(text))\n",
        "\n",
        "print(\"\\nVal Targets Dimensions:\")\n",
        "for i, summary in enumerate(val_targets):\n",
        "    print(f\"Summary {i + 1} length:\", len(summary))\n",
        "\n",
        "print(\"\\nTest Texts Dimensions:\")\n",
        "for i, text in enumerate(test_texts):\n",
        "    print(f\"Text {i + 1} length:\", len(text))\n",
        "\n",
        "print(\"\\nTest Targets Dimensions:\")\n",
        "for i, summary in enumerate(test_targets):\n",
        "    print(f\"Summary {i + 1} length:\", len(summary))"
      ],
      "metadata": {
        "id": "v-XlaAC3HJ4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ev80D_6xQCFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Represent text as word embeddings\n",
        "\n",
        "# -Word embeddings examples are Word2Vec, GloVe, FastText, and represent each word as a dense vector.\n",
        "# follow https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# List of train texts\n",
        "train_preprocessed_text = []\n",
        "for article_text in train_texts:  # Use train_texts instead of looping through train_data\n",
        "    train_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "# List of val texts\n",
        "val_preprocessed_text = []\n",
        "for article_text in val_texts:  # Use val_texts instead of looping through val_data\n",
        "    val_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "# List of test texts\n",
        "test_preprocessed_text = []\n",
        "for article_text in test_texts:  # Use test_texts instead of looping through test_data\n",
        "    test_preprocessed_text.append(pad_or_truncate_text(article_text, max_length))\n",
        "\n",
        "\n",
        "# Build vocabulary from both texts and summaries\n",
        "all_texts_and_summaries = train_preprocessed_text + val_preprocessed_text + test_preprocessed_text + train_summaries + val_summaries + test_summaries\n",
        "word_freq = {}  # A dictionary to store word frequencies\n",
        "\n",
        "for sentence in all_texts_and_summaries:\n",
        "    for word in sentence:\n",
        "        if word not in word_freq:\n",
        "            word_freq[word] = 0\n",
        "        word_freq[word] += 1\n",
        "\n",
        "# Sort words based on frequency in descending order\n",
        "sorted_words = sorted(word_freq.keys(), key=lambda x: word_freq[x], reverse=True)\n",
        "\n",
        "# Take only the top vocab_size - 1 words, and add an OOV token\n",
        "vocab_size = len(sorted_words)\n",
        "sorted_words = sorted_words[:vocab_size - 1] + ['<OOV>', '<SOS>']\n",
        "\n",
        "# Build word_to_num based on the sorted_words list\n",
        "word_to_num = {word: i for i, word in enumerate(sorted_words)}\n",
        "word_to_num['<OOV>'] = word_to_num.get('<OOV>', vocab_size - 2)  # The second to last index\n",
        "\n",
        "\n",
        "def sentence_to_indices(sentence):\n",
        "    return [word_to_num.get(word, word_to_num['<OOV>']) for word in sentence]\n",
        "\n",
        "class wordembedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(wordembedding, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embeddings(inputs)\n",
        "\n",
        "# Create the model and the optimizer\n",
        "word_model = wordembedding(len(sorted_words), embedding_dim)  # Use len(sorted_words) as vocab_size\n",
        "optimizer = optim.SGD(word_model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ... (previous code remains unchanged)\n",
        "\n",
        "# Loop over train_targets and generate embeddings for summaries\n",
        "train_summary_embeddings_list = []\n",
        "for summary in train_targets:\n",
        "    summary_idxs = [word_to_num.get(word, word_to_num['<OOV>']) for word in summary]  # Get the index of each word in the summary\n",
        "    summary_idxs = torch.tensor(summary_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    embeddings = word_model(summary_idxs)\n",
        "    train_summary_embeddings_list.append(embeddings)  # Store the embeddings\n",
        "\n",
        "# Loop over val_targets and generate embeddings for summaries\n",
        "val_summary_embeddings_list = []\n",
        "for summary in val_targets:\n",
        "    summary_idxs = [word_to_num.get(word, word_to_num['<OOV>']) for word in summary]  # Get the index of each word in the summary\n",
        "    summary_idxs = torch.tensor(summary_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    embeddings = word_model(summary_idxs)\n",
        "    val_summary_embeddings_list.append(embeddings)  # Store the embeddings\n",
        "\n",
        "# Loop over test_targets and generate embeddings for summaries\n",
        "test_summary_embeddings_list = []\n",
        "for summary in test_targets:\n",
        "    summary_idxs = [word_to_num.get(word, word_to_num['<OOV>']) for word in summary]  # Get the index of each word in the summary\n",
        "    summary_idxs = torch.tensor(summary_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    embeddings = word_model(summary_idxs)\n",
        "    test_summary_embeddings_list.append(embeddings)  # Store the embeddings\n",
        "\n",
        "train_summary_embeddings_tensor = torch.stack(train_summary_embeddings_list)\n",
        "val_summary_embeddings_tensor = torch.stack(val_summary_embeddings_list)\n",
        "test_summary_embeddings_tensor = torch.stack(test_summary_embeddings_list)\n",
        "\n",
        "train_text_embeddings_list = []\n",
        "val_text_embeddings_list = []\n",
        "test_text_embeddings_list = []\n",
        "\n",
        "# Loop over train_preprocessed_text and generate embeddings for texts\n",
        "for sentence in train_preprocessed_text:\n",
        "    sentence_idxs = [word_to_num[word] for word in sentence]  # Get the index of each word in the sentence\n",
        "    sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    embeddings = word_model(sentence_idxs)\n",
        "    train_text_embeddings_list.append(embeddings)  # Store the embeddings\n",
        "\n",
        "# Loop over val_preprocessed_text and generate embeddings for texts\n",
        "for sentence in val_preprocessed_text:\n",
        "    sentence_idxs = [word_to_num[word] for word in sentence]  # Get the index of each word in the sentence\n",
        "    sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    embeddings = word_model(sentence_idxs)\n",
        "    val_text_embeddings_list.append(embeddings)  # Store the embeddings\n",
        "\n",
        "# Loop over test_preprocessed_text and generate embeddings for texts\n",
        "for sentence in test_preprocessed_text:\n",
        "    sentence_idxs = [word_to_num[word] for word in sentence]  # Get the index of each word in the sentence\n",
        "    sentence_idxs = torch.tensor(sentence_idxs, dtype=torch.long)  # Convert the list to tensor outside the loop\n",
        "    embeddings = word_model(sentence_idxs)\n",
        "    test_text_embeddings_list.append(embeddings)  # Store the embeddings\n",
        "\n",
        "# Convert lists to PyTorch tensors for texts\n",
        "train_text_embeddings_tensor = torch.stack(train_text_embeddings_list).to(torch.long)\n",
        "val_text_embeddings_tensor = torch.stack(val_text_embeddings_list).to(torch.long)\n",
        "test_text_embeddings_tensor = torch.stack(test_text_embeddings_list).to(torch.long)\n",
        "\n",
        "\n",
        "# print embeddings\n",
        "print(\"Train Summary Embeddings:\")\n",
        "print(train_summary_embeddings_tensor)\n",
        "\n",
        "print(\"Validation Summary Embeddings:\")\n",
        "print(val_summary_embeddings_tensor)\n",
        "\n",
        "print(\"Test Summary Embeddings:\")\n",
        "print(test_summary_embeddings_tensor)\n",
        "\n",
        "print(\"Train Text Embeddings:\")\n",
        "print(train_text_embeddings_tensor)\n",
        "\n",
        "print(\"Validation Text Embeddings:\")\n",
        "print(val_text_embeddings_tensor)\n",
        "\n",
        "print(\"Test Text Embeddings:\")\n",
        "print(test_text_embeddings_tensor)\n",
        "\n",
        "train_summary_embeddings = train_summary_embeddings_tensor\n",
        "val_summary_embeddings = val_summary_embeddings_tensor\n",
        "test_summary_embeddings = test_summary_embeddings_tensor\n",
        "train_text_embeddings = train_text_embeddings_tensor\n",
        "val_text_embeddings = val_text_embeddings_tensor\n",
        "test_text_embeddings = test_text_embeddings_tensor\n",
        "\n",
        "\n",
        "print(train_text_embeddings.size())\n",
        "print(train_summary_embeddings.size())\n",
        "print(val_text_embeddings.size())\n",
        "print(val_summary_embeddings.size())\n",
        "print(test_text_embeddings.size())\n",
        "print(test_summary_embeddings.size())\n",
        "print(\"Shape of train_text_embeddings_tensor:\", train_text_embeddings_tensor.shape)\n",
        "print(\"Shape of train_summary_embeddings_tensor:\", train_summary_embeddings_tensor.shape)\n",
        "print(\"Shape of val_text_embeddings_tensor:\", val_text_embeddings_tensor.shape)\n",
        "print(\"Shape of val_summary_embeddings_tensor:\", val_summary_embeddings_tensor.shape)\n",
        "print(\"Shape of test_text_embeddings_tensor:\", test_text_embeddings_tensor.shape)\n",
        "print(\"Shape of test_summary_embeddings_tensor:\", test_summary_embeddings_tensor.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gM2YPK7UNX3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Assuming you have the following variables containing the embeddings:\n",
        "train_text_embeddings_tensor, train_summary_embeddings_tensor\n",
        "val_text_embeddings_tensor, val_summary_embeddings_tensor\n",
        "test_text_embeddings_tensor, test_summary_embeddings_tensor\n",
        "\n",
        "# Define a custom dataset for your training data\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, text_embeddings, summary_embeddings):\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.summary_embeddings = summary_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.text_embeddings[idx], self.summary_embeddings[idx]\n",
        "\n",
        "# Batch size for training, validation, and test data loaders\n",
        "batch_size = 32\n",
        "\n",
        "# Create instances of the custom dataset for your training, validation, and test data\n",
        "train_dataset = CustomDataset(train_text_embeddings_tensor, train_summary_embeddings_tensor)\n",
        "val_dataset = CustomDataset(val_text_embeddings_tensor, val_summary_embeddings_tensor)\n",
        "test_dataset = CustomDataset(test_text_embeddings_tensor, test_summary_embeddings_tensor)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "\n",
        "print(\"Train Loader Dimensions:\")\n",
        "for inputs, targets in train_loader:\n",
        "    print(\"Input Tensor Dimensions:\", inputs.size())\n",
        "    print(\"Target Tensor Dimensions:\", targets.size())\n",
        "    break  # Print dimensions for only the first batch\n",
        "\n",
        "print(\"\\nValidation Loader Dimensions:\")\n",
        "for inputs, targets in val_loader:\n",
        "    print(\"Input Tensor Dimensions:\", inputs.size())\n",
        "    print(\"Target Tensor Dimensions:\", targets.size())\n",
        "    break  # Print dimensions for only the first batch\n",
        "\n",
        "print(\"\\nTest Loader Dimensions:\")\n",
        "for inputs, targets in test_loader:\n",
        "    print(\"Input Tensor Dimensions:\", inputs.size())\n",
        "    print(\"Target Tensor Dimensions:\", targets.size())\n",
        "    break  # Print dimensions for only the first batch\n",
        "\n",
        "print(\"Train Loader Number of Samples:\", len(train_loader.dataset))\n",
        "print(\"Validation Loader Number of Samples:\", len(val_loader.dataset))\n",
        "print(\"Test Loader Number of Samples:\", len(test_loader.dataset))\n"
      ],
      "metadata": {
        "id": "oSpYKI-oC7PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########  NN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib as plt\n",
        "\n",
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, embedded_input):\n",
        "        encoder_outputs, (hidden_state, cell_state) = self.rnn(embedded_input)\n",
        "        return encoder_outputs, (hidden_state, cell_state)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "\n",
        "    def forward(self, hidden_tuple, encoder_outputs):\n",
        "        hidden, _ = hidden_tuple\n",
        "        max_len = encoder_outputs.size(1)\n",
        "        h = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n",
        "        encoder_outputs = encoder_outputs.transpose(1, 2)\n",
        "        attn_scores = F.softmax(torch.bmm(h, self.attn(encoder_outputs)), dim=2)\n",
        "        context = torch.bmm(attn_scores, encoder_outputs)\n",
        "        return context\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.rnn = nn.LSTM(embedding_dim + hidden_dim, hidden_dim, batch_first=True)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, embedded_target, hidden, encoder_outputs):\n",
        "        context = self.attention(hidden, encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded_target, context.unsqueeze(1).repeat(1, embedded_target.size(1), 1)), dim=2)\n",
        "        decoder_outputs, hidden = self.rnn(rnn_input, hidden)\n",
        "        output_sequence = F.log_softmax(self.fc(decoder_outputs), dim=2)\n",
        "        return output_sequence, hidden\n",
        "\n",
        "\n",
        "class Seq2SeqAttentionWithEmbedding(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, src_embedding_dim, tgt_embedding_dim, hidden_dim):\n",
        "        super(Seq2SeqAttentionWithEmbedding, self).__init__()\n",
        "        self.embedding_source = nn.Embedding(src_vocab_size, src_embedding_dim)\n",
        "        self.embedding_target = nn.Embedding(tgt_vocab_size, tgt_embedding_dim)\n",
        "        self.encoder = Encoder(src_embedding_dim, hidden_dim)\n",
        "        self.decoder = Decoder(tgt_embedding_dim, hidden_dim, tgt_vocab_size)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "\n",
        "    def forward(self, src_input, tgt_input):\n",
        "        embedded_source = self.embedding_source(src_input)\n",
        "        embedded_target = self.embedding_target(tgt_input)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(embedded_source)\n",
        "        context = self.attention(hidden, encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded_target, context), dim=2)\n",
        "\n",
        "        decoder_outputs, hidden = self.decoder.rnn(rnn_input, hidden)\n",
        "        output_sequence = F.log_softmax(self.decoder.fc(decoder_outputs), dim=2)\n",
        "        return output_sequence, hidden\n",
        "\n",
        "\n",
        "\n",
        "src_vocab_size = len(word_to_num)\n",
        "tgt_vocab_size = len(word_to_num)  # Use the same vocabulary for both source and target for now, you can create separate ones if needed\n",
        "model = Seq2SeqAttentionWithEmbedding(src_vocab_size, tgt_vocab_size, embedding_dim, embedding_dim, hidden_dim)\n",
        "\n",
        "\n",
        "# loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "j4RIQoRb-9o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm as tqdm_module  # Importing tqdm with a different name to avoid conflicts\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "\n",
        "train_accuracy_list = []\n",
        "val_accuracy_list = []\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    word_model.train()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with tqdm_module(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
        "        for batch_idx, (src_input, tgt_input) in enumerate(train_loader):\n",
        "            src_input, tgt_input = src_input.to(device), tgt_input.to(device).long()\n",
        "\n",
        "            batch_size = src_input.size(0)\n",
        "\n",
        "            # Calculate max_tgt_length dynamically for the current batch\n",
        "            max_tgt_length = tgt_input.size(1)\n",
        "\n",
        "            sos_token_idx = torch.tensor([word_to_num['<SOS>']], dtype=torch.long).to(device)  # Shape: [1]\n",
        "\n",
        "            sos_token_emb = word_model(sos_token_idx)  # Shape: [1, embedding_dim]\n",
        "            sos_token_emb = sos_token_emb.repeat(batch_size, 1, 1)  # Shape: [batch_size, 1, embedding_dim]\n",
        "\n",
        "            tgt_input_emb = word_model(tgt_input)  # Assuming tgt_input is 2D [batch_size, sequence_length]\n",
        "            tgt_input_with_sos = torch.cat([sos_token_emb, tgt_input_emb], dim=1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output_sequence, _ = word_model(src_input, tgt_input_with_sos[:, :-1, :])\n",
        "            total_samples += tgt_input.size(0) * tgt_input.size(1)  # Calculate before reshaping tgt_input\n",
        "            tgt_input = tgt_input.contiguous().view(-1).to(torch.long)\n",
        "\n",
        "            loss = criterion(output_sequence, tgt_input)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "            # calculate training accuracy\n",
        "            _, predicted = torch.max(output_sequence, 2)  # Use dim=2 to find max along time steps\n",
        "            correct_predictions += (predicted == tgt_input.view_as(predicted)).sum().item()\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    train_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    train_accuracy_list.append(train_accuracy)\n",
        "    train_loss_list.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    word_model.eval()\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src_input, tgt_input) in enumerate(val_loader):\n",
        "            src_input, tgt_input = src_input.to(device).to(torch.long), tgt_input.to(device).to(torch.long)\n",
        "\n",
        "            output_sequence, _ = word_model(src_input, tgt_input[:, :-1])\n",
        "\n",
        "            output_sequence = output_sequence.contiguous().view(-1, output_sequence.size(-1))\n",
        "            total_samples += tgt_input.size(0) * tgt_input.size(1)  # Calculate before reshaping tgt_input\n",
        "            tgt_input = tgt_input.contiguous().view(-1).to(torch.long)  # Convert to 1D LongTensor\n",
        "\n",
        "            loss = criterion(output_sequence, tgt_input)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = torch.max(output_sequence, 1)\n",
        "            correct_predictions += (predicted == tgt_input).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    val_accuracy = correct_predictions / total_samples\n",
        "\n",
        "    val_accuracy_list.append(val_accuracy)\n",
        "    val_loss_list.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Graphs every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(range(1, epoch + 2), train_accuracy_list, label=\"Train Accuracy\")\n",
        "        plt.plot(range(1, epoch + 2), val_accuracy_list, label=\"Validation Accuracy\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Training and Validation Accuracy vs. Epoch\")\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(range(1, epoch + 2), train_loss_list, label=\"Train Loss\")\n",
        "        plt.plot(range(1, epoch + 2), val_loss_list, label=\"Validation Loss\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.title(\"Training and Validation Loss vs. Epoch\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Final graphs\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), train_accuracy_list, label=\"Train Accuracy\")\n",
        "plt.plot(range(1, num_epochs + 1), val_accuracy_list, label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Accuracy vs. Epoch\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), train_loss_list, label=\"Train Loss\")\n",
        "plt.plot(range(1, num_epochs + 1), val_loss_list, label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Validation Loss vs. Epoch\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "ZgAas3pnM_nJ",
        "outputId": "b7ffd253-a5b3-4fb7-aefe-10881a2d7cf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20:   0%|          | 0/3 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-404-e21793d15412>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0msos_token_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msos_token_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: [batch_size, 1, embedding_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mtgt_input_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_input\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming tgt_input is 2D [batch_size, sequence_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtgt_input_with_sos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msos_token_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_input_emb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-402-8976c764124d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Create the model and the optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    }
  ]
}